{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fractured_dir = '/kaggle/input/bone-fracture-detection-using-xrays/archive (6)/train/fractured/'\n",
    "train_not_fractured_dir = '/kaggle/input/bone-fracture-detection-using-xrays/archive (6)/train/not fractured/'\n",
    "\n",
    "val_fractured_dir = '/kaggle/input/bone-fracture-detection-using-xrays/archive (6)/val/fractured/'\n",
    "val_not_fractured_dir = '/kaggle/input/bone-fracture-detection-using-xrays/archive (6)/val/not fractured/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(directory):\n",
    "    return len(os.listdir(directory))\n",
    "\n",
    "train_fractured_count = count_images(train_fractured_dir)\n",
    "train_not_fractured_count = count_images(train_not_fractured_dir)\n",
    "val_fractured_count = count_images(val_fractured_dir)\n",
    "val_not_fractured_count = count_images(val_not_fractured_dir)\n",
    "\n",
    "print(f'Train Fractured: {train_fractured_count}')\n",
    "print(f'Train Not Fractured: {train_not_fractured_count}')\n",
    "print(f'Validation Fractured: {val_fractured_count}')\n",
    "print(f'Validation Not Fractured: {val_not_fractured_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Train Fractured', 'Train Not Fractured', 'Validation Fractured', 'Validation Not Fractured']\n",
    "counts = [train_fractured_count, train_not_fractured_count, val_fractured_count, val_not_fractured_count]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=labels, y=counts)\n",
    "plt.title('Count of Images in Each Folder')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(counts, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribution of Images in Each Folder')\n",
    "plt.axis('equal')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(folder, count=5):\n",
    "    images = os.listdir(folder)\n",
    "    selected_images = np.random.choice(images, count, replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, img_name in enumerate(selected_images):\n",
    "        img_path = os.path.join(folder, img_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  \n",
    "        \n",
    "        plt.subplot(1, count, i + 1)\n",
    "        plt.imshow(img, cmap='gray') \n",
    "        plt.axis('off')\n",
    "        plt.title(img_name)\n",
    "    plt.show()\n",
    "\n",
    "print(\"Sample Images from Train Fractured:\")\n",
    "display_images(train_fractured_dir)\n",
    "\n",
    "print(\"Sample Images from Train Not Fractured:\")\n",
    "display_images(train_not_fractured_dir)\n",
    "\n",
    "print(\"Sample Images from Validation Fractured:\")\n",
    "display_images(val_fractured_dir)\n",
    "\n",
    "print(\"Sample Images from Validation Not Fractured:\")\n",
    "display_images(val_not_fractured_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractured_images = os.listdir(train_fractured_dir)\n",
    "not_fractured_images = os.listdir(train_not_fractured_dir)\n",
    "\n",
    "df_fractured = pd.DataFrame({\n",
    "    'filename': fractured_images,\n",
    "    'label': 'fractured'\n",
    "})\n",
    "df_not_fractured = pd.DataFrame({\n",
    "    'filename': not_fractured_images,\n",
    "    'label': 'not fractured'\n",
    "})\n",
    "\n",
    "df = pd.concat([df_fractured, df_not_fractured], ignore_index=True)\n",
    "\n",
    "df['file_path'] = df.apply(lambda row: os.path.join(train_fractured_dir if row['label'] == 'fractured' else train_not_fractured_dir, row['filename']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['file_path', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(df[['file_path']], df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resampled = pd.DataFrame(X_resampled, columns=['file_path'])\n",
    "df_resampled['label'] = y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClass distribution after oversampling:\")\n",
    "print(df_resampled['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Provides time-related functions, such as sleep and measuring execution time.\n",
    "\n",
    "import shutil  # Used for high-level file operations like copying, moving, and deleting files and directories.\n",
    "import pathlib  # Provides an object-oriented approach to handling file system paths.\n",
    "import itertools  # Contains functions for iterators, such as permutations, combinations, and infinite loops\n",
    "from PIL import Image  # Part of the Pillow library; used for opening, manipulating, and saving image files.\n",
    "\n",
    "\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print ('check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resampled.to_csv('bone_fracture.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_new, temp_df_new = train_test_split(\n",
    "    df_resampled,\n",
    "    train_size=0.8,  \n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    stratify=df_resampled['label']  \n",
    ")\n",
    "\n",
    "valid_df_new, test_df_new = train_test_split(\n",
    "    temp_df_new,\n",
    "    test_size=0.5,  \n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    stratify=temp_df_new['label'] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 16\n",
    "img_size = (224, 224)\n",
    "channels = 1 \n",
    "img_shape = (img_size[0], img_size[1], channels)\n",
    "\n",
    "tr_gen = ImageDataGenerator(rescale=1./255)  \n",
    "ts_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_gen_new = tr_gen.flow_from_dataframe(\n",
    "    train_df_new,\n",
    "    x_col='file_path',  \n",
    "    y_col='label',     \n",
    "    target_size=img_size,\n",
    "    class_mode='binary',  \n",
    "    color_mode='grayscale', \n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "valid_gen_new = ts_gen.flow_from_dataframe(\n",
    "    valid_df_new,\n",
    "    x_col='file_path',  \n",
    "    y_col='label',     \n",
    "    target_size=img_size,\n",
    "    class_mode='binary',  \n",
    "    color_mode='grayscale', \n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_gen_new = ts_gen.flow_from_dataframe(\n",
    "    test_df_new,\n",
    "    x_col='file_path', \n",
    "    y_col='label',    \n",
    "    target_size=img_size,\n",
    "    class_mode='binary',  \n",
    "    color_mode='grayscale', \n",
    "    shuffle=False,  \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 16\n",
    "img_size = (224, 224)\n",
    "channels = 3\n",
    "img_shape = (img_size[0], img_size[1], channels)\n",
    "\n",
    "tr_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "ts_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_gen_new = tr_gen.flow_from_dataframe(\n",
    "    train_df_new,\n",
    "    x_col='file_path',  \n",
    "    y_col='label',     \n",
    "    target_size=img_size,\n",
    "    class_mode='binary',  \n",
    "    color_mode='rgb', \n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "valid_gen_new = ts_gen.flow_from_dataframe(\n",
    "    valid_df_new,\n",
    "    x_col='file_path',  \n",
    "    y_col='label',     \n",
    "    target_size=img_size,\n",
    "    class_mode='binary',  \n",
    "    color_mode='rgb', \n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_gen_new = ts_gen.flow_from_dataframe(\n",
    "    test_df_new,\n",
    "    x_col='file_path', \n",
    "    y_col='label',    \n",
    "    target_size=img_size,\n",
    "    class_mode='binary',  \n",
    "    color_mode='rgb', \n",
    "    shuffle=False,  \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, GaussianNoise, Input, MultiHeadAttention\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_vgg16_modified_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    base_model = VGG16(weights='imagenet', input_tensor=inputs, include_top=False)\n",
    "    \n",
    "    for layer in base_model.layers[:-4]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    x = base_model.output\n",
    "    \n",
    "    attention_output = MultiHeadAttention(num_heads=8, key_dim=x.shape[-1])(x, x)\n",
    "    x = GaussianNoise(0.25)(attention_output)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = GaussianNoise(0.25)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "cnn_modified_model = create_vgg16_modified_model(input_shape)\n",
    "\n",
    "cnn_modified_model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                           loss='binary_crossentropy',\n",
    "                           metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn_modified_model.fit(\n",
    "    train_gen_new,\n",
    "    validation_data=valid_gen_new,\n",
    "    epochs=3,  \n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_gen_new.classes  \n",
    "predictions = cnn_modified_model.predict(test_gen_new)  \n",
    "predicted_labels = (predictions > 0.5).astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(test_labels, predicted_labels, target_names=list(test_gen_new.class_indices.keys()))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fractured', 'Fractured'], yticklabels=['Not Fractured', 'Fractured'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "    return model\n",
    "\n",
    "input_shape = (224, 224, 1)  \n",
    "num_classes = 1  \n",
    "\n",
    "cnn_model = create_cnn_model(input_shape)\n",
    "\n",
    "cnn_model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',  \n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn_model.fit(\n",
    "    train_gen_new,\n",
    "    validation_data=valid_gen_new,\n",
    "    epochs=10,  \n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_gen_new.classes  \n",
    "predictions = cnn_model.predict(test_gen_new)  \n",
    "predicted_labels = (predictions > 0.5).astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(test_labels, predicted_labels, target_names=list(test_gen_new.class_indices.keys()))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fractured', 'Fractured'], yticklabels=['Not Fractured', 'Fractured'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 16\n",
    "img_size = (224, 224)\n",
    "channels = 1 \n",
    "img_shape = (img_size[0], img_size[1], channels)\n",
    "\n",
    "tr_gen = ImageDataGenerator(rescale=1./255)  \n",
    "ts_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_gen_new = tr_gen.flow_from_dataframe(\n",
    "    train_df_new,\n",
    "    x_col='file_path',  \n",
    "    y_col='label',     \n",
    "    target_size=img_size,\n",
    "    class_mode='binary',  \n",
    "    color_mode='rgb', \n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "valid_gen_new = ts_gen.flow_from_dataframe(\n",
    "    valid_df_new,\n",
    "    x_col='file_path',  \n",
    "    y_col='label',     \n",
    "    target_size=img_size,\n",
    "    class_mode='binary',  \n",
    "    color_mode='rgb', \n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_gen_new = ts_gen.flow_from_dataframe(\n",
    "    test_df_new,\n",
    "    x_col='file_path', \n",
    "    y_col='label',    \n",
    "    target_size=img_size,\n",
    "    class_mode='binary',  \n",
    "    color_mode='rgb', \n",
    "    shuffle=False,  \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "    return model\n",
    "\n",
    "input_shape = (224, 224, 3)  \n",
    "num_classes = 1  \n",
    "\n",
    "cnn_model = create_cnn_model(input_shape)\n",
    "\n",
    "cnn_model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',  \n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn_model.fit(\n",
    "    train_gen_new,\n",
    "    validation_data=valid_gen_new,\n",
    "    epochs=10,  \n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_gen_new.classes  \n",
    "predictions = cnn_model.predict(test_gen_new)  \n",
    "predicted_labels = (predictions > 0.5).astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fractured', 'Fractured'], yticklabels=['Not Fractured', 'Fractured'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(test_labels, predicted_labels, target_names=['Not Fractured', 'Fractured'])\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
